{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b62b4e4-d10a-49f9-b8ed-e96b6722d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate \n",
    "from collections import Counter\n",
    "\n",
    "#Exercise\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb22deed-b528-4c82-aa88-cb7ae7ee460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Exercise\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "925eda34-e2c9-4108-bf62-ceee472858d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n",
    "def preprocess_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75d9ee75-a5f7-47eb-beca-0ee32a2b28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46374971-e364-4377-8d65-a68318041c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n",
    "tokenized_dataset = [preprocess_tokens(doc) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f2bc3f3-7a7e-4443-b554-a5a2ac593595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6876795b-45d8-406d-b281-ef456719579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in doc.split() if word in word2vec_model.wv], axis=0)\n",
    "    for doc in dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e2820ff-0518-4251-893c-1729cce49dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1c498fc-bc42-4ede-b9b9-6f99212f6248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f2469-dc1f-48f6-81ad-28509509da6b",
   "metadata": {},
   "source": [
    "##### Do the Purity differ when applying text preprocessing before vectorization?\n",
    "###### The purity is higher after applying preprocessing steps as it reduces noise and irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0235d-db7e-4212-98d5-9b226c2492ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
